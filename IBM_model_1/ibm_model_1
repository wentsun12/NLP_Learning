import collections
from decimal import Decimal as D
from operator import itemgetter

def word_tokenize(sent):
    return sent.split(' ')

## PART1: 读取平行语料库数据，分别读取两个不同的文件。 第一个文件里的每一行对应第二个文件里的每一行
pcorpus = dict()
lines_lang1 = open("data_lang1.txt", "r").readlines()
lines_lang2 = open("data_lang2.txt", "r").readlines()

for line1, line2 in zip(lines_lang1, lines_lang2):
    # 分词
    sentence1 = tuple(word_tokenize("NULL " + line1.strip("\n")))
    sentence2 = tuple(word_tokenize("NULL " + line2.strip("\n")))
    pcorpus[sentence1] = sentence2

## PART2: TODO
##        定义模型参数，以及初始化。 在这里，我们最后要求出来的是 p(word_i_lang1 | word_j_lang2), 也就是模型的参数。
##        在这里定义变量 translation_probs， 并给它一个初识的值。

f_key = set()
for (es, fs) in pcorpus.items():
    for f in fs:
        f_key.add(f)

translation_probs = collections.defaultdict(lambda: D(1/len(f_key)))

num_epochs = 10
for i in range(num_epochs):
    ## TODO 此处为核心的代码部分，需要循环平行语料库，同时不断更新模型参数。
    ##      适当参考给定的参考资料，并花时间思考问题具体怎么解决。这里的核心是不断更新translation_probs。 初次接触会发现有一定的难度，这很正常。

    count = collections.defaultdict(D)
    total = collections.defaultdict(D)
    s_total = collections.defaultdict(D)
    for (es, fs) in pcorpus.items():
        for e in es:
            s_total[e] = 0
            for f in fs:
                s_total[e] += translation_probs[(e, f)]
        for e in es:
            for f in fs:
                count[(e, f)] += translation_probs[(e, f)] / s_total[e]
                total[f] += translation_probs[(e, f)] / s_total[e]

    for (e, f) in count.keys():
        translation_probs[(e, f)] = count[(e, f)] / total[f]

## PART 4: 打印结果

print()
print("{:<40}{:>40}".format("t(lang1|lang2)", "Value"))
print("--------------------------------------------------------------------------------")
iterations = 0
for ((lang1_word, lang2_word), value) in sorted(translation_probs.items(), key=itemgetter(1), reverse=True):
    print("{:<40}{:>40.2}".format("t(%s|%s)" % (lang1_word, lang2_word), value))